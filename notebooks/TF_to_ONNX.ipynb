{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahaLOgxyzACW"
      },
      "source": [
        "# Convert tf.keras model to ONNX\n",
        "\n",
        "This tutorial shows \n",
        "- how to convert tf.keras model to ONNX from the saved model file or the source code directly. \n",
        "- comparison of the execution time of the inference on CPU between tf.keras model and ONNX converted model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmnzNRTkzaYq"
      },
      "source": [
        "## Install ONNX dependencies\n",
        "- `tf2onnx` provides a tool to convert TensorFlow model to ONNX\n",
        "- `onnxruntime` is used to run inference on a saved ONNX model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7VIFntKUh0R"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqq tf2onnx\n",
        "!pip install -Uqq onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7TJluNyz8k0"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-UfszPPVf9P0"
      },
      "outputs": [],
      "source": [
        "import tf2onnx\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eo3f1Zn0S3F"
      },
      "source": [
        "### Get a sample model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3R81akF_hDEL"
      },
      "outputs": [],
      "source": [
        "core = tf.keras.applications.ResNet50(include_top=True, input_shape=(224, 224, 3))\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"image_input\")\n",
        "preprocess = tf.keras.applications.resnet50.preprocess_input(inputs)\n",
        "outputs = core(preprocess, training=False)\n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQg5cN910Z6q"
      },
      "source": [
        "## Convert to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-friv_fMk79",
        "outputId": "b25b7903-cac8-4c84-d4f5-81e83f8013db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first layer name: image_input\n",
            "last layer name: resnet50\n"
          ]
        }
      ],
      "source": [
        "num_layers = len(model.layers)\n",
        "print(f'first layer name: {model.layers[0].name}')\n",
        "print(f'last layer name: {model.layers[num_layers-1].name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBGQxHHz0dGP"
      },
      "source": [
        "### Conversion\n",
        "\n",
        "`opset` in `tf2onnx.convert.from_keras` is the ONNX Op version. You can find the full list which TF Ops are convertible to ONNX Ops [[here](https://github.com/onnx/tensorflow-onnx/blob/master/support_status.md)].\n",
        "\n",
        "there are two ways to convert TensorFlow model to ONNX\n",
        "- `tf2onnx.convert.from_keras` to convert programatically\n",
        "- `tf2onnx.convert` CLI to convert a saved TensorFlow model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_MAEoy9j0QRQ"
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "\n",
        "input_signature = [tf.TensorSpec([None, 224, 224, 3], tf.float32, name='image_input')]\n",
        "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=15)\n",
        "onnx.save(onnx_model, \"my_model.onnx\")\n",
        "\n",
        "# model.save('my_model')\n",
        "# !python -m tf2onnx.convert --saved-model my_model --output my_model.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2-aNpahQMVR"
      },
      "source": [
        "## Test TF vs ONNX model with dummy data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt5lsQoUQXOo"
      },
      "source": [
        "### Generate dummy data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ceqZH2KbPznx"
      },
      "outputs": [],
      "source": [
        "dummy_inputs = tf.random.normal((32, 224, 224, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8DR47zeQZHI"
      },
      "source": [
        "### Test original TF model with dummy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL8Lw9H8QbT7",
        "outputId": "0d3017b3-a9e9-4ff9-9246-73a33fd1f4aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loop, best of 5: 4.32 s per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "model.predict(dummy_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smFa5VWjTNLb",
        "outputId": "1565f632-0b01-4079-838c-6d6f8ec867e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[3.15686048e-05 3.87393957e-04 1.79542520e-04 ... 1.30086582e-05\n",
            "  9.79110773e-05 3.81577667e-03]\n",
            " [3.48336362e-05 4.16526280e-04 1.93989443e-04 ... 1.55881808e-05\n",
            "  1.13529946e-04 4.16510738e-03]\n",
            " [3.33409043e-05 4.33546113e-04 1.97361689e-04 ... 1.53988676e-05\n",
            "  1.12037051e-04 4.24840674e-03]\n",
            " ...\n",
            " [2.89980180e-05 3.86187923e-04 1.73962238e-04 ... 1.27737330e-05\n",
            "  1.00923535e-04 4.02137777e-03]\n",
            " [3.22057713e-05 3.88624350e-04 1.79953189e-04 ... 1.43914149e-05\n",
            "  1.02170088e-04 3.93211702e-03]\n",
            " [3.32495474e-05 4.18914686e-04 1.89737257e-04 ... 1.49391553e-05\n",
            "  1.15772651e-04 4.33536153e-03]]\n",
            "tf.Tensor(\n",
            "[664 664 664 664 664 664 664 664 664 664 664 664 664 664 664 664 664 664\n",
            " 851 664 664 664 664 664 664 851 664 664 664 664 664 664], shape=(32,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "tf_preds = model.predict(dummy_inputs)\n",
        "print(tf_preds)\n",
        "print(tf.argmax(tf_preds, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqhi458k0fkM"
      },
      "source": [
        "### Test converted ONNX model with dummy data\n",
        "\n",
        "If you want to inference with GPU, then you can do so by setting `providers=[\"CUDAExecutionProvider\"]` in `ort.InferenceSession`.\n",
        "\n",
        "The first parameter in `sess.run` is set to `None`, and that means all the outputs of the model will be retrieved. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1ELVBwrn0-Cf"
      },
      "outputs": [],
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "sess = ort.InferenceSession(\"my_model.onnx\") # providers=[\"CUDAExecutionProvider\"])\n",
        "np_dummy_inputs = dummy_inputs.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jszhyR15SJaE",
        "outputId": "fd9d4ce6-ec46-4b9d-f699-d6764d3640eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loop, best of 5: 3.64 s per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit \n",
        "sess.run(None, {\"image_input\": np_dummy_inputs})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax6opk4ENmlK",
        "outputId": "18ec2dee-af04-42f9-bebc-13043ae787d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([[3.1568543e-05, 3.8739358e-04, 1.7954242e-04, ..., 1.3008658e-05,\n",
            "        9.7910888e-05, 3.8157741e-03],\n",
            "       [3.4834065e-05, 4.1653065e-04, 1.9399117e-04, ..., 1.5588314e-05,\n",
            "        1.1353097e-04, 4.1651078e-03],\n",
            "       [3.3341144e-05, 4.3354733e-04, 1.9736330e-04, ..., 1.5398955e-05,\n",
            "        1.1203749e-04, 4.2484212e-03],\n",
            "       ...,\n",
            "       [2.8998174e-05, 3.8618816e-04, 1.7396275e-04, ..., 1.2773765e-05,\n",
            "        1.0092379e-04, 4.0213722e-03],\n",
            "       [3.2205990e-05, 3.8862476e-04, 1.7995379e-04, ..., 1.4391498e-05,\n",
            "        1.0217058e-04, 3.9321054e-03],\n",
            "       [3.3249766e-05, 4.1891521e-04, 1.8973851e-04, ..., 1.4939254e-05,\n",
            "        1.1577292e-04, 4.3353694e-03]], dtype=float32)]\n",
            "[664 664 664 664 664 664 664 664 664 664 664 664 664 664 664 664 664 664\n",
            " 851 664 664 664 664 664 664 851 664 664 664 664 664 664]\n"
          ]
        }
      ],
      "source": [
        "ort_preds = sess.run(None, {\"image_input\": np_dummy_inputs})\n",
        "print(ort_preds)\n",
        "print(np.argmax(ort_preds[0], axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPu6kdNnU8Y6"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "We did a simple experiments with dummy dataset of 32 batch size. The default behaviour of `timeit` is to measure the average of the cell execution time with 7 times of repeat ([`timeit`'s default behaviour](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit))\n",
        "\n",
        "\n",
        "The TF implementation of the ResNet50 took about 4.32s while the ONNX converted model took about 3.64s on average for the the inference job. So it is clear ONNX converted model is much faster on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXx5Mr2VWobP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TF to ONNX.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
