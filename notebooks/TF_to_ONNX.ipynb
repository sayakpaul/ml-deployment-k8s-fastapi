{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahaLOgxyzACW"
      },
      "source": [
        "# Convert tf.keras model to ONNX\n",
        "\n",
        "This tutorial shows \n",
        "- how to convert tf.keras model to ONNX from the saved model file or the source code directly. \n",
        "- comparison of the execution time of the inference on CPU between tf.keras model and ONNX converted model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmnzNRTkzaYq"
      },
      "source": [
        "## Install ONNX dependencies\n",
        "- `tf2onnx` provides a tool to convert TensorFlow model to ONNX\n",
        "- `onnxruntime` is used to run inference on a saved ONNX model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7VIFntKUh0R"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqq tf2onnx\n",
        "!pip install -Uqq onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7TJluNyz8k0"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-UfszPPVf9P0"
      },
      "outputs": [],
      "source": [
        "import tf2onnx\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eo3f1Zn0S3F"
      },
      "source": [
        "### Get a sample model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R81akF_hDEL",
        "outputId": "adf38696-1803-4c7a-a46c-a60456032eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "102973440/102967424 [==============================] - 2s 0us/step\n",
            "102981632/102967424 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.applications.ResNet50(include_top=True, input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQg5cN910Z6q"
      },
      "source": [
        "## Convert to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-friv_fMk79",
        "outputId": "aa07f614-3f48-4a97-abbe-8e264e0eb4ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first layer name: input_1\n",
            "last layer name: predictions\n"
          ]
        }
      ],
      "source": [
        "num_layers = len(model.layers)\n",
        "print(f'first layer name: {model.layers[0].name}')\n",
        "print(f'last layer name: {model.layers[num_layers-1].name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBGQxHHz0dGP"
      },
      "source": [
        "### Conversion\n",
        "\n",
        "`opset` in `tf2onnx.convert.from_keras` is the ONNX Op version. You can find the full list which TF Ops are convertible to ONNX Ops [[here](https://github.com/onnx/tensorflow-onnx/blob/master/support_status.md)]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MAEoy9j0QRQ",
        "outputId": "cc921c1d-0809-4a9d-9017-db0c93a805e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
          ]
        }
      ],
      "source": [
        "import onnx\n",
        "\n",
        "input_signature = [tf.TensorSpec([None, 224, 224, 3], tf.float32, name='input_1')]\n",
        "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=15)\n",
        "onnx.save(onnx_model, \"my_model.onnx\")\n",
        "\n",
        "# model.save('my_model')\n",
        "# !python -m tf2onnx.convert --saved-model my_model --output my_model.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2-aNpahQMVR"
      },
      "source": [
        "## Test TF vs ONNX model with dummy data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt5lsQoUQXOo"
      },
      "source": [
        "### Generate dummy data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ceqZH2KbPznx"
      },
      "outputs": [],
      "source": [
        "dummy_inputs = tf.random.normal((32, 224, 224, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8DR47zeQZHI"
      },
      "source": [
        "### Test original TF model with dummy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL8Lw9H8QbT7",
        "outputId": "d584c360-9302-425b-dfd1-ab479cd66cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loop, best of 5: 5.43 s per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "model(dummy_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smFa5VWjTNLb",
        "outputId": "a06b06f9-3425-4927-c743-97fc978e2ff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[7.7266595e-06 1.2607347e-04 3.5493996e-04 ... 3.9009101e-06\n",
            "  6.4691136e-05 2.9123765e-03]\n",
            " [1.0709026e-05 1.3441169e-04 3.6555584e-04 ... 5.0727599e-06\n",
            "  6.6014669e-05 3.2316628e-03]\n",
            " [8.6340870e-06 1.3047195e-04 3.3897307e-04 ... 4.2910137e-06\n",
            "  6.6816639e-05 3.1956623e-03]\n",
            " ...\n",
            " [1.0132569e-05 1.4086883e-04 3.5038366e-04 ... 5.2608348e-06\n",
            "  6.4157386e-05 3.2055001e-03]\n",
            " [6.7479391e-06 1.2649213e-04 2.8330638e-04 ... 4.4393978e-06\n",
            "  6.4049018e-05 3.0844919e-03]\n",
            " [8.6630262e-06 1.2813542e-04 2.9517489e-04 ... 5.8516707e-06\n",
            "  7.2770934e-05 3.3875057e-03]], shape=(32, 1000), dtype=float32)\n",
            "tf.Tensor(\n",
            "[783 783 783 783 783 783 783 783 783 783 783 783 783 783 783 783 783 783\n",
            " 783 783 783 783 783 783 783 783 783 783 783 783 783 783], shape=(32,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "tf_preds = model(dummy_inputs)\n",
        "print(tf_preds)\n",
        "print(tf.argmax(tf_preds, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqhi458k0fkM"
      },
      "source": [
        "### Test converted ONNX model with dummy data\n",
        "\n",
        "If you want to inference with GPU, then you can do so by setting `providers=[\"CUDAExecutionProvider\"]` in `ort.InferenceSession`.\n",
        "\n",
        "The first parameter in `sess.run` is set to `None`, and that means all the outputs of the model will be retrieved. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1ELVBwrn0-Cf"
      },
      "outputs": [],
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "sess = ort.InferenceSession(\"my_model.onnx\") # providers=[\"CUDAExecutionProvider\"])\n",
        "np_dummy_inputs = dummy_inputs.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jszhyR15SJaE",
        "outputId": "d1c99858-8be7-4757-a8cc-08e266927247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loop, best of 5: 3.97 s per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit \n",
        "sess.run(None, {\"input_1\": np_dummy_inputs})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax6opk4ENmlK",
        "outputId": "61f72025-716d-4edf-b14a-0d73ef908c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([[7.7266932e-06, 1.2607401e-04, 3.5494129e-04, ..., 3.9009115e-06,\n",
            "        6.4691383e-05, 2.9123744e-03],\n",
            "       [1.0709014e-05, 1.3441198e-04, 3.6555488e-04, ..., 5.0727685e-06,\n",
            "        6.6014931e-05, 3.2316626e-03],\n",
            "       [8.6341424e-06, 1.3047228e-04, 3.3897266e-04, ..., 4.2910538e-06,\n",
            "        6.6816901e-05, 3.1956607e-03],\n",
            "       ...,\n",
            "       [1.0132635e-05, 1.4086922e-04, 3.5038497e-04, ..., 5.2608593e-06,\n",
            "        6.4157597e-05, 3.2054954e-03],\n",
            "       [6.7478845e-06, 1.2649220e-04, 2.8330489e-04, ..., 4.4393792e-06,\n",
            "        6.4049040e-05, 3.0844780e-03],\n",
            "       [8.6630653e-06, 1.2813594e-04, 2.9517466e-04, ..., 5.8516916e-06,\n",
            "        7.2771159e-05, 3.3875043e-03]], dtype=float32)]\n",
            "[783 783 783 783 783 783 783 783 783 783 783 783 783 783 783 783 783 783\n",
            " 783 783 783 783 783 783 783 783 783 783 783 783 783 783]\n"
          ]
        }
      ],
      "source": [
        "ort_preds = sess.run(None, {\"input_1\": np_dummy_inputs})\n",
        "print(ort_preds)\n",
        "print(np.argmax(ort_preds[0], axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPu6kdNnU8Y6"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "We did a simple experiments with dummy dataset of 32 batch size. The default behaviour of `timeit` is to measure the average of the cell execution time with 7 times of repeat ([`timeit`'s default behaviour](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit))\n",
        "\n",
        "\n",
        "The TF implementation of the ResNet50 took about 5.43s while the ONNX converted model took about 3.97s on average for the the inference job. So it is clear ONNX converted model is much faster on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXx5Mr2VWobP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TF to ONNX.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
